{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from src.dataset import SPFastaDatasetBinary, SPFastaDatasetBinaryWithTokenizedCategory\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = SPFastaDatasetBinaryWithTokenizedCategory(\"data/train.fasta\")\n",
    "# dataset_train = Dataset.from_pandas(ds.data).with_format(\"torch\", device=device)\n",
    "# ds = SPFastaDatasetBinaryWithTokenizedCategory(\"data/test.fasta\")\n",
    "# dataset_test = Dataset.from_pandas(ds.data).with_format(\"torch\", device=device)\n",
    "# del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20290/20290 [00:00<00:00, 183476.44it/s]\n",
      "100%|██████████| 20290/20290 [00:00<00:00, 534029.63it/s]\n",
      "100%|██████████| 8811/8811 [00:00<00:00, 259114.96it/s]\n",
      "100%|██████████| 8811/8811 [00:00<00:00, 734434.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data_with_size_uniform(path, size=1000):\n",
    "    ds = SPFastaDatasetBinaryWithTokenizedCategory(path)\n",
    "    neg_labels = ds.data.loc[ds.data['labels'] == 0][:size//2]\n",
    "    pos_labels = ds.data.loc[ds.data['labels'] == 1][:size//2]\n",
    "    combi = pd.concat((neg_labels, pos_labels))\n",
    "    return Dataset.from_pandas(combi).with_format(\"torch\", device=device)\n",
    "\n",
    "dataset_train = load_data_with_size_uniform(\"data/train.fasta\", 100000)\n",
    "dataset_test  = load_data_with_size_uniform(\"data/test.fasta\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3f644966ca4a8e9203ee622673c6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/20290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ea94e0799a47508c93f954db786068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/8811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "dataset_train = dataset_train.cast_column('labels', ClassLabel(num_classes = 2, names=[\"NO_SP\", \"SP\"]))\n",
    "dataset_test = dataset_test.cast_column('labels', ClassLabel(num_classes = 2, names=[\"NO_SP\", \"SP\"]))\n",
    "dataset_train[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1db7591bf8a4ecfb0b3a8305a9871af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e597f2450e64ff398ba62ef348db621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'labels', 'uniprot_ac', 'kingdom', 'type', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 20290\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'labels', 'uniprot_ac', 'kingdom', 'type', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 8811\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", device=\"cpu\")\n",
    "dataset_train = dataset_train.map(lambda x: tokenizer(x['text'], return_tensors=\"pt\", padding='max_length', max_length=81, truncation=True), batched=True)\n",
    "dataset_test = dataset_test.map(lambda x: tokenizer(x['text'], return_tensors=\"pt\", padding='max_length', max_length=81, truncation=True), batched=True)\n",
    "dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "pre = evaluate.load(\"precision\")\n",
    "rec = evaluate.load(\"recall\")\n",
    "f1m = evaluate.load(\"f1\")\n",
    "\n",
    "metrics = [acc, pre, rec, f1m]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y = eval_pred\n",
    "    x = logits.argmax(-1)\n",
    "    return {k: v for metric in [m.compute(predictions=x, references=y) for m in metrics] for k, v in metric.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class ProtBertSequenceClassification(nn.Module):\n",
    "    def __init__(self, device: str=device) -> None:\n",
    "        super(ProtBertSequenceClassification, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            #\"Rostlab/prot_bert\",\n",
    "            \"microsoft/MiniLM-L12-H384-uncased\",\n",
    "            num_labels=2,\n",
    "            label2id = {\n",
    "                'NO_SP': 0,\n",
    "                'SP': 1\n",
    "            },\n",
    "            id2label= {\n",
    "                0: 'NO_SP',\n",
    "                1: 'SP'\n",
    "            }).to(device)\n",
    "\n",
    "        self.loss_fn = nn.functional.binary_cross_entropy\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, token_type_ids=None):\n",
    "        outputs = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    def loss(self, x, weights=None):\n",
    "        logits = self.forward(x).logits[:,1]\n",
    "        pred = logits.softmax(-1)\n",
    "        label = x['labels']\n",
    "        return self.loss_fn(pred, label.float(), weight=weights)\n",
    "\n",
    "model = ProtBertSequenceClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FocalLoss, compute class weights and create a custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = weights\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6493, 2.1747], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=dataset_train['labels'].cpu().numpy())\n",
    "class_weights = torch.tensor(class_weights).to(device).float()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        labels = inputs.get('labels')\n",
    "\n",
    "        loss_fn = FocalLoss(weights=class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# batch_size = 8\n",
    "epochs = 50\n",
    "logging_steps=1000\n",
    "eval_steps=10000\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./model',\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    logging_first_step=True,\n",
    "    weight_decay=0.0001,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    fp16=True,\n",
    "    optim='adamw_torch',\n",
    "    remove_unused_columns=True,\n",
    "    auto_find_batch_size=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748474b7bb614caeae90170887a40aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1488, 'learning_rate': 1.9999842333464724e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1737, 'learning_rate': 1.9842333464722114e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1713, 'learning_rate': 1.968498226251478e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1741, 'learning_rate': 1.9527315727236894e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1357, 'learning_rate': 1.9369806858494287e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0985, 'learning_rate': 1.92121403232164e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0746, 'learning_rate': 1.9054473787938513e-05, 'epoch': 2.36}\n",
      "{'loss': 0.077, 'learning_rate': 1.8897437918801735e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0747, 'learning_rate': 1.8739771383523847e-05, 'epoch': 3.15}\n",
      "{'loss': 0.0607, 'learning_rate': 1.858210484824596e-05, 'epoch': 3.55}\n",
      "{'loss': 0.0648, 'learning_rate': 1.8424438312968072e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d90265d5d445aa40f2719028c73b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.047781217843294144, 'eval_accuracy': 0.971966859607309, 'eval_precision': 0.9169491525423729, 'eval_recall': 0.8789601949634444, 'eval_f1': 0.8975528826213189, 'eval_runtime': 30.9282, 'eval_samples_per_second': 284.886, 'eval_steps_per_second': 35.631, 'epoch': 3.94}\n",
      "{'loss': 0.0604, 'learning_rate': 1.8266771777690185e-05, 'epoch': 4.34}\n",
      "{'loss': 0.0587, 'learning_rate': 1.8109105242412298e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0635, 'learning_rate': 1.795159637366969e-05, 'epoch': 5.12}\n",
      "{'loss': 0.0558, 'learning_rate': 1.779408750492708e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0525, 'learning_rate': 1.7636420969649194e-05, 'epoch': 5.91}\n",
      "{'loss': 0.05, 'learning_rate': 1.7478754434371306e-05, 'epoch': 6.31}\n",
      "{'loss': 0.0513, 'learning_rate': 1.7321245565628696e-05, 'epoch': 6.7}\n",
      "{'loss': 0.0423, 'learning_rate': 1.716357903035081e-05, 'epoch': 7.09}\n",
      "{'loss': 0.0417, 'learning_rate': 1.70060701616082e-05, 'epoch': 7.49}\n",
      "{'loss': 0.0432, 'learning_rate': 1.684840362633031e-05, 'epoch': 7.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42be3fa29b49484bb73c0f197069aee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03126443922519684, 'eval_accuracy': 0.9800249687890137, 'eval_precision': 0.9189833200953137, 'eval_recall': 0.9398862713241267, 'eval_f1': 0.9293172690763052, 'eval_runtime': 30.5459, 'eval_samples_per_second': 288.451, 'eval_steps_per_second': 36.077, 'epoch': 7.88}\n",
      "{'loss': 0.0378, 'learning_rate': 1.6690737091052424e-05, 'epoch': 8.28}\n",
      "{'loss': 0.041, 'learning_rate': 1.6533070555774536e-05, 'epoch': 8.67}\n",
      "{'loss': 0.0403, 'learning_rate': 1.637540402049665e-05, 'epoch': 9.07}\n",
      "{'loss': 0.0338, 'learning_rate': 1.621805281828932e-05, 'epoch': 9.46}\n",
      "{'loss': 0.0352, 'learning_rate': 1.6060386283011432e-05, 'epoch': 9.85}\n",
      "{'loss': 0.0356, 'learning_rate': 1.5902877414268822e-05, 'epoch': 10.25}\n",
      "{'loss': 0.0343, 'learning_rate': 1.5745210878990935e-05, 'epoch': 10.64}\n",
      "{'loss': 0.039, 'learning_rate': 1.5587702010248328e-05, 'epoch': 11.04}\n",
      "{'loss': 0.0336, 'learning_rate': 1.543003547497044e-05, 'epoch': 11.43}\n",
      "{'loss': 0.0294, 'learning_rate': 1.5272368939692553e-05, 'epoch': 11.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408cf7a47960413697f4dbf8fa0a6e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023840567097067833, 'eval_accuracy': 0.9878560889796845, 'eval_precision': 0.9599018003273322, 'eval_recall': 0.9528838342810723, 'eval_f1': 0.9563799429270281, 'eval_runtime': 29.3298, 'eval_samples_per_second': 300.411, 'eval_steps_per_second': 37.573, 'epoch': 11.82}\n",
      "{'loss': 0.0362, 'learning_rate': 1.5114702404414664e-05, 'epoch': 12.22}\n",
      "{'loss': 0.0352, 'learning_rate': 1.4957035869136779e-05, 'epoch': 12.61}\n",
      "{'loss': 0.0297, 'learning_rate': 1.4799369333858891e-05, 'epoch': 13.01}\n",
      "{'loss': 0.0316, 'learning_rate': 1.464186046511628e-05, 'epoch': 13.4}\n",
      "{'loss': 0.0312, 'learning_rate': 1.4484193929838392e-05, 'epoch': 13.8}\n",
      "{'loss': 0.0315, 'learning_rate': 1.4326527394560506e-05, 'epoch': 14.19}\n",
      "{'loss': 0.0275, 'learning_rate': 1.4168860859282619e-05, 'epoch': 14.58}\n",
      "{'loss': 0.0431, 'learning_rate': 1.401135199054001e-05, 'epoch': 14.98}\n",
      "{'loss': 0.034, 'learning_rate': 1.3853843121797399e-05, 'epoch': 15.37}\n",
      "{'loss': 0.0275, 'learning_rate': 1.3696176586519511e-05, 'epoch': 15.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6277b034017a459fadb0fd451b39b4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.022756921127438545, 'eval_accuracy': 0.9868346385200317, 'eval_precision': 0.9657477025898078, 'eval_recall': 0.9390739236393176, 'eval_f1': 0.9522240527182867, 'eval_runtime': 29.4347, 'eval_samples_per_second': 299.34, 'eval_steps_per_second': 37.439, 'epoch': 15.77}\n",
      "{'loss': 0.0285, 'learning_rate': 1.3538510051241624e-05, 'epoch': 16.16}\n",
      "{'loss': 0.0286, 'learning_rate': 1.3381001182499016e-05, 'epoch': 16.55}\n",
      "{'loss': 0.0279, 'learning_rate': 1.3223334647221128e-05, 'epoch': 16.95}\n",
      "{'loss': 0.0254, 'learning_rate': 1.306566811194324e-05, 'epoch': 17.34}\n",
      "{'loss': 0.0239, 'learning_rate': 1.2908001576665355e-05, 'epoch': 17.74}\n",
      "{'loss': 0.0269, 'learning_rate': 1.2750335041387468e-05, 'epoch': 18.13}\n",
      "{'loss': 0.0268, 'learning_rate': 1.259266850610958e-05, 'epoch': 18.53}\n",
      "{'loss': 0.0242, 'learning_rate': 1.2435001970831693e-05, 'epoch': 18.92}\n",
      "{'loss': 0.0268, 'learning_rate': 1.2277493102089081e-05, 'epoch': 19.31}\n",
      "{'loss': 0.0268, 'learning_rate': 1.2119984233346473e-05, 'epoch': 19.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502b991238d947ab83e728ba16c2ff71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.017343735322356224, 'eval_accuracy': 0.9872886165020996, 'eval_precision': 0.9612530915086562, 'eval_recall': 0.9471974004874086, 'eval_f1': 0.9541734860883797, 'eval_runtime': 29.9772, 'eval_samples_per_second': 293.923, 'eval_steps_per_second': 36.761, 'epoch': 19.71}\n",
      "{'loss': 0.0238, 'learning_rate': 1.1962317698068587e-05, 'epoch': 20.1}\n",
      "{'loss': 0.0279, 'learning_rate': 1.1804808829325975e-05, 'epoch': 20.5}\n",
      "{'loss': 0.0251, 'learning_rate': 1.1647142294048088e-05, 'epoch': 20.89}\n",
      "{'loss': 0.0258, 'learning_rate': 1.148963342530548e-05, 'epoch': 21.28}\n"
     ]
    }
   ],
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = dataset_test.select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "sample = samples[torch.randint(len(samples), (1,))]\n",
    "with torch.no_grad():\n",
    "    pred = model(**sample).logits.argmax(-1)\n",
    "pred-sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d243332e9de4391940140d5c4935d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m----> 5\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend((\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, l\u001b[38;5;241m.\u001b[39mitem()))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for batch in tqdm(torch.utils.data.DataLoader(samples, batch_size=8)):\n",
    "    pred = model(**batch).logits.argmax(-1)\n",
    "    for p, l in zip(pred, batch['labels']):\n",
    "        res.append((p.item(), l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred  label\n",
       "0     0        1789\n",
       "1     0           3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res, columns=['pred', 'label']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: check if this is correct\n",
    "loss_weights = torch.tensor([neg_weight if x[0] == 1 else pos_weight for x in batch['label']])\n",
    "loss_weights = torch.tensor([neg_weight, pos_weight]).to(device) # or this\n",
    "model_v = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "# Classical training loop:\n",
    "import pickle\n",
    "loss_log = []\n",
    "epoch0 = 0\n",
    "if os.path.exists(\"model/temp.state\"):\n",
    "    with open(\"model/temp.state\", \"rb\") as f:\n",
    "        state = pickle.load(f)\n",
    "        loss_log = state['loss_log']\n",
    "        epoch0 = state['epoch']\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "model.train()\n",
    "batches = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(epoch0, epochs):\n",
    "    for i, batch in enumerate(tqdm(batches)):\n",
    "        class_weights = torch.tensor([neg_weight if x[0] == 1 else pos_weight for x in batch['labels']]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch, weights=class_weights)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_log.append(loss.cpu().detach().item())\n",
    "        if (i+1) % (len(batches) // 20) == 0:\n",
    "            print(np.mean(loss_log[-5000:]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
